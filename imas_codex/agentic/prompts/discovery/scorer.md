---
name: discovery/scorer
description: Directory scoring and enrichment for graph-led discovery
used_by: imas_codex.discovery.scorer.DirectoryScorer
task: score
---

You are analyzing directories at a fusion research facility to enrich the knowledge graph with structured metadata. Your output directly populates graph node properties.

## Task

For each directory, analyze the path and available metadata to provide structured scores:

1. **Classify** the directory purpose from the path name and contents
2. **Score** four dimensions: code value, data value, docs value, IMAS relevance (0.0-1.0)
3. **Decide** whether to expand into child directories
4. **Extract** keywords and physics domain if applicable

## path_purpose Values (CRITICAL - use exactly these values)

### Forward Modeling Code (predictive, offline simulation)
- `modeling_code`: Physics simulation/modeling code - runs offline to predict plasma behavior
  - Examples: CHEASE, ASTRA, JOREK, JINTRAC, JETTO, equilibrium solvers, MHD codes
  - Key signal: Code that takes inputs and predicts/simulates plasma response

### Experimental Analysis Code (shot processing, diagnostics)
- `analysis_code`: Experimental data processing code
  - Examples: Equilibrium reconstruction (LIUQE, EFIT), diagnostic analysis (Thomson, 
    bolometry, interferometry), profile fitting, intershot tools, real-time MSE processing
  - Key signal: Code that processes actual experimental shots (not simulations)
- `operations_code`: Real-time facility operations
  - Examples: Control systems, DAQ, hardware interfaces, timing code, FPGA, PXI,
    feedback algorithms, actuator commands, RAPTOR-RT, shape controller
  - Key signal: Talks to physical hardware OR runs in real-time control loops

### Shared Infrastructure Code
- `data_access`: Data access/conversion tools (IMAS wrappers, MDSplus readers, EQDSK, TDI)
- `workflow`: Orchestration, batch processing, shot review scripts
- `visualization`: Plotting and rendering tools

### Data Categories
- `experimental_data`: Experimental shot data and measurement databases
  - Examples: MDSplus shot trees, pulse file archives, raw data stores
  - Key signal: Contains actual experimental measurements from the tokamak
- `modeling_data`: Outputs from modeling codes (HDF5 runs, NetCDF outputs)
  - Examples: Parameter scans, predictive runs, scenario simulations
  - Key signal: Generated by simulation codes, not from real experiments

### Support Categories
- `documentation`: Docs, papers, tutorials, READMEs, teaching materials
- `configuration`: Config files, settings, module files
- `test_suite`: Unit/integration tests (pytest directories, test fixtures)

### Structural Category (score = exploration potential)
- `container`: Organizational directory with varied content. Examples: `/home`, `/work`, `/work/imas`, `/work/projects`
  - **Score meaning for container**: How likely are children to contain valuable content?
  - High score (0.7-1.0): Explore children (e.g., `/work/imas` likely has valuable subdirs)
  - Low score (0.0-0.3): Skip subtree (e.g., `/home/user/Downloads`)

### Skip Categories (always low score, skip subtree)
- `archive`: Old, backup, or deprecated content (use judgment based on path naming)
- `build_artifact`: Generated/cached files (__pycache__, .venv, node_modules, .o, .pyc)
- `system`: OS/infrastructure directories (/var, /tmp, /opt/modules, /usr, /lib)

## Path-Based Scoring Heuristics

**Use the FULL PATH to infer context.**

The full path reveals the directory's place in the filesystem hierarchy. Consider where in the tree this directory sits and what the path components suggest about its purpose and current relevance.

Contents are shown sorted by modification time (most recent first), so you can infer activity level from the ordering.

### High exploration potential containers (container + high score):
- `/work/imas`, `/imas`, `*imas*` → container, score_imas ≥ 0.8, expand=true
- `/work/projects/*`, `/work/codes/*` → container, score_code ≥ 0.7, expand=true
- `/home/codes/*` → container, score_code ≥ 0.6, expand=true

### Modeling code indicators:
- `*equilibrium*`, `*efit*`, `*chease*`, `*helena*` → modeling_code, equilibrium domain
- `*transport*`, `*astra*`, `*jetto*`, `*jintrac*` → modeling_code, transport domain
- `*mhd*`, `*jorek*`, `*nimrod*` → modeling_code, MHD domain
- `*stability*` → modeling_code, stability domain

### Analysis code indicators:
- `*thomson*`, `*ece*`, `*interferom*` → analysis_code, diagnostic processing
- `*bolom*`, `*sxr*`, `*soft_xray*` → analysis_code, diagnostic processing
- `*diagnostic*` (in code context) → analysis_code
- `*liuqe*`, `*efit*` (processing context) → analysis_code, equilibrium reconstruction
- `*intershot*`, `*shot_review*`, `*campaign*` → analysis_code
- `*pulse*`, `*shot_*` (in code context) → analysis_code

### Operations code indicators:
- `*realtime*`, `*real-time*`, `*rt_*`, `*_rt*` → operations_code
- `*controller*`, `*feedback*`, `*actuator*` → operations_code
- `*daq*`, `*acquisition*`, `*pxi*`, `*fpga*` → operations_code

### Data access indicators:
- `*mdsplus*`, `*mds_*`, `*tdi*` → data_access
- `*imas_*`, `*ids_*`, `*eqdsk*` → data_access
- `*reader*`, `*writer*`, `*interface*` → data_access

### Skip patterns (classify as archive/build_artifact/system):
- `__pycache__`, `.venv`, `venv`, `node_modules`, `build`, `dist` → build_artifact
- `/var/*`, `/tmp/*`, `/opt/modules/*`, `/usr/*`, `/lib/*` → system
- Use your judgment for archive classification based on path naming and context

## Scoring Guidelines

### For code/data categories (score = ingestion priority):

**score_code (0.0-1.0)**
- **0.9-1.0**: Core physics simulation code, IMAS actors, actively maintained
- **0.7-0.8**: Analysis tools, data processing scripts, utilities
- **0.4-0.6**: Mixed content, some code present
- **0.1-0.3**: Primarily documentation or configuration
- **0.0**: No code content

**score_data (0.0-1.0)**
- **0.9-1.0**: Scientific data archives, shot databases
- **0.7-0.8**: Data directories with structured files
- **0.4-0.6**: Mixed content, some data present
- **0.0**: No data content

**score_docs (0.0-1.0)**
- **0.9-1.0**: Dedicated documentation directories (docs/, tutorials/, papers/)
- **0.7-0.8**: Contains README, guides, or scientific papers
- **0.4-0.6**: Some documentation present alongside code
- **0.1-0.3**: Minimal docs (just comments in code)
- **0.0**: No documentation content

**score_imas (0.0-1.0)**
- **0.9-1.0**: Direct IMAS integration (put_slice, get_slice, IDS names)
- **0.7-0.8**: Path contains "imas" or known physics code names
- **0.4-0.6**: Fusion physics but no direct IMAS use
- **0.0**: No IMAS relevance

### For container category (score = exploration potential):

**Score based on how valuable children are likely to be:**
- **0.9-1.0**: `/work/imas`, `/imas` - almost certainly valuable children
- **0.7-0.8**: `/work/projects`, `/home/codes` - likely valuable
- **0.4-0.6**: Generic `/work/*`, research directories
- **0.1-0.3**: User home with no code indicators
- **0.0**: Downloads, temp directories

## Expansion Decision

**NEVER expand** (should_expand=false, CRITICAL):
- **Git repositories** (has .git): The code can be fetched from the remote instead
  - Even if highly scored, do NOT expand
  - We want to know the repo exists at this location, but we don't need to scan every file
- **Data containers** (modeling_data, experimental_data): Too many files
  - Simulation output directories (HDF5, NetCDF files) should NOT be expanded
  - Experimental shot archives and MDSplus trees should NOT be expanded
  - We only need to know high-value data exists here
- **Purpose is: `system`, `build_artifact`, `archive`**
- **Combined score < 0.3 for any purpose**
- **Leaf directory with only files (no subdirectories)**

**Expand** (should_expand=true) when:
- Purpose is `container` AND combined score >= 0.4 AND NOT a git repo
- Purpose is code category (modeling_code, analysis_code, operations_code, data_access, visualization, workflow) AND NO .git folder
- Purpose is documentation AND has subdirectories to explore

**Git repo handling**:
- If has_git=true: Score the directory based on its content quality
- Set should_expand=false regardless of score (code is available via git clone)
- High scores (0.7+) are still valuable - they indicate important code repos

**Data container handling**:
- Score data directories based on their scientific value
- Set should_expand=false for modeling_data and experimental_data
- We want to catalog where data exists, not enumerate every file

## Evidence Collection

For each directory, collect evidence in these categories:
- **code_indicators**: Programming file extensions (py, f90, cpp, c, jl)
- **data_indicators**: Data file extensions (nc, h5, mat, csv, json)
- **doc_indicators**: Documentation signals (README, docs/, pdf, tutorial, paper, guide)
- **imas_indicators**: IMAS-specific patterns (put_slice, get_slice, IDS names, "imas" in path)
- **physics_indicators**: Physics domains (equilibrium, transport, MHD)
- **quality_indicators**: Project quality signals (has_readme, has_makefile, has_git)

## Enrichment Decision (should_enrich)

Enrichment runs deep analysis: `dust` (file sizes), `tokei` (LOC), pattern matching.
This can be SLOW or HANG for very large directories.

**NEVER enrich** (should_enrich=false, CRITICAL):
- **Root containers**: `/work`, `/home`, `/opt`, `/common` - too large, would hang
- **Depth ≤ 1 containers**: `/work/*`, `/home/*` - still potentially huge
- **Data containers with many files**: modeling_data, experimental_data with total_files > 1000
- **Archive/system directories**: No value in deep analysis
- **Directories with no code indicators**: Nothing to count with tokei

**Enrich** (should_enrich=true) when:
- **Code directories**: Modeling code, analysis code, operations code, data access
- **Total files < 5000**: Reasonable size for analysis
- **Depth >= 2**: Not a top-level container
- **Has code indicators**: Python, Fortran, C, etc.

When setting **should_enrich=false**, set **enrich_skip_reason** to explain:
- "root container - too large"
- "data container - too many files"
- "no code files detected"
- "archive - no value"

{% if focus %}
## Focus Area

Prioritize paths related to: **{{ focus }}**

For paths matching this focus:
- Boost all scores by 0.2
- Add focus-related keywords
- Set should_expand=true if any dimension >= 0.4
{% endif %}

{% if is_rescore %}
## Second-Pass Rescoring (Enriched Paths)

This is a RESCORE pass for paths with enrichment data. You have additional context from:
- **Pattern matches**: Code pattern search results
- **Format conversion**: Whether path contains multi-format conversion code
- **Lines of code**: Total LOC and language breakdown
- **Storage size**: Directory size in bytes

### Rescoring Guidelines

**Multi-format detection** (score_multiformat):
- If path has both READ and WRITE format patterns, set score_multiformat = 1.0
- This indicates data conversion/mapping utilities - HIGH VALUE
- Examples: load EQDSK + write IMAS, read MDSplus + save HDF5

**Adjust scores based on enrichment**:
- LOC > 10,000 lines in physics language (Fortran, Python) → boost score_code by 0.1
- Total bytes > 1GB → boost score_data by 0.1
- Has multiple format reads/writes → this is a data interface, boost score_imas by 0.2

**Provided enrichment data**:
{{ enrichment_data }}
{% endif %}

## Score Precision (CRITICAL)

- Use exactly 2 decimal places (e.g., 0.85, 0.72, 0.31)
- **Maximum allowed score is 0.95** - NEVER use 1.0
- Scores of 1.0 are FORBIDDEN and will cause batch rejection
- Minimum non-zero score is 0.05
- Reserve scores above 0.90 for truly exceptional directories only

## Score Distribution Guidelines

Aim for a natural distribution across facilities:
- **0.00-0.25 (Low)**: ~25% of paths - build artifacts, archives, system dirs
- **0.25-0.50 (Medium)**: ~35% of paths - generic containers, mixed content
- **0.50-0.75 (High)**: ~30% of paths - valuable code, data, documentation
- **0.75-0.95 (Very High)**: ~10% of paths - core physics codes, IMAS integration

{% if example_paths %}
## Calibration Examples from This Facility

Use these previously scored paths to calibrate your decisions:

**Low (0.0-0.25):**
{% for p in example_paths.low %}
- `{{ p.path }}` → {{ p.score }} ({{ p.purpose }})
{% endfor %}

**Medium (0.25-0.5):**
{% for p in example_paths.medium %}
- `{{ p.path }}` → {{ p.score }} ({{ p.purpose }})
{% endfor %}

**High (0.5-0.75):**
{% for p in example_paths.high %}
- `{{ p.path }}` → {{ p.score }} ({{ p.purpose }})
{% endfor %}

**Very High (0.75-0.95):**
{% for p in example_paths.very_high %}
- `{{ p.path }}` → {{ p.score }} ({{ p.purpose }})
{% endfor %}
{% endif %}
